{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read post data into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('influencers_data.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    #text = text.replace('\\%','')\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    #text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('...see more', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = \" \".join(filter(lambda x:x[0]!=\"@\", text.split()))\n",
    "    return text\n",
    "\n",
    "corpus = data['content'].apply(lambda x: clean_text(x)).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    filters='\"$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Turn tokenized sentences into training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad Input Sequences\n",
    "max_sequence_length = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre'))\n",
    "\n",
    "# Create predictors and labels\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wingp/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 34s 384ms/step - loss: 7.5950 - accuracy: 0.0212\n",
      "Epoch 2/100\n",
      "83/83 [==============================] - 38s 463ms/step - loss: 7.7285 - accuracy: 0.0208\n",
      "Epoch 3/100\n",
      "83/83 [==============================] - 33s 398ms/step - loss: 7.2442 - accuracy: 0.0268\n",
      "Epoch 4/100\n",
      "83/83 [==============================] - 33s 403ms/step - loss: 6.5308 - accuracy: 0.0382\n",
      "Epoch 5/100\n",
      "83/83 [==============================] - 33s 397ms/step - loss: 6.1416 - accuracy: 0.0472\n",
      "Epoch 6/100\n",
      "83/83 [==============================] - 33s 403ms/step - loss: 5.8132 - accuracy: 0.0548\n",
      "Epoch 7/100\n",
      "83/83 [==============================] - 33s 395ms/step - loss: 5.4375 - accuracy: 0.0650\n",
      "Epoch 8/100\n",
      "83/83 [==============================] - 33s 398ms/step - loss: 5.0229 - accuracy: 0.0892\n",
      "Epoch 9/100\n",
      "83/83 [==============================] - 33s 400ms/step - loss: 4.5972 - accuracy: 0.1153\n",
      "Epoch 10/100\n",
      "83/83 [==============================] - 30s 358ms/step - loss: 4.1428 - accuracy: 0.1682\n",
      "Epoch 11/100\n",
      "83/83 [==============================] - 31s 369ms/step - loss: 3.6356 - accuracy: 0.2222\n",
      "Epoch 12/100\n",
      "83/83 [==============================] - 33s 392ms/step - loss: 3.1864 - accuracy: 0.2789\n",
      "Epoch 13/100\n",
      "83/83 [==============================] - 34s 406ms/step - loss: 2.6996 - accuracy: 0.3692\n",
      "Epoch 14/100\n",
      "83/83 [==============================] - 33s 402ms/step - loss: 2.2601 - accuracy: 0.4592\n",
      "Epoch 15/100\n",
      "83/83 [==============================] - 33s 404ms/step - loss: 1.9083 - accuracy: 0.5374\n",
      "Epoch 16/100\n",
      "83/83 [==============================] - 33s 397ms/step - loss: 1.5628 - accuracy: 0.6134\n",
      "Epoch 17/100\n",
      "83/83 [==============================] - 33s 402ms/step - loss: 1.2933 - accuracy: 0.6833\n",
      "Epoch 18/100\n",
      "83/83 [==============================] - 33s 400ms/step - loss: 1.0742 - accuracy: 0.7336\n",
      "Epoch 19/100\n",
      "83/83 [==============================] - 33s 401ms/step - loss: 0.8405 - accuracy: 0.8012\n",
      "Epoch 20/100\n",
      "83/83 [==============================] - 34s 406ms/step - loss: 0.6792 - accuracy: 0.8428\n",
      "Epoch 21/100\n",
      "83/83 [==============================] - 33s 402ms/step - loss: 0.5143 - accuracy: 0.8949\n",
      "Epoch 22/100\n",
      "83/83 [==============================] - 34s 407ms/step - loss: 0.4093 - accuracy: 0.9172\n",
      "Epoch 23/100\n",
      "83/83 [==============================] - 33s 402ms/step - loss: 0.3464 - accuracy: 0.9297\n",
      "Epoch 24/100\n",
      "83/83 [==============================] - 34s 407ms/step - loss: 0.2687 - accuracy: 0.9562\n",
      "Epoch 25/100\n",
      "83/83 [==============================] - 33s 400ms/step - loss: 0.3660 - accuracy: 0.9161\n",
      "Epoch 26/100\n",
      "83/83 [==============================] - 33s 398ms/step - loss: 0.4353 - accuracy: 0.8946\n",
      "Epoch 27/100\n",
      "83/83 [==============================] - 38s 463ms/step - loss: 0.2567 - accuracy: 0.9520\n",
      "Epoch 28/100\n",
      "83/83 [==============================] - 35s 425ms/step - loss: 0.1552 - accuracy: 0.9735\n",
      "Epoch 29/100\n",
      "83/83 [==============================] - 40s 481ms/step - loss: 0.1097 - accuracy: 0.9860\n",
      "Epoch 30/100\n",
      "83/83 [==============================] - 35s 420ms/step - loss: 0.0810 - accuracy: 0.9917\n",
      "Epoch 31/100\n",
      "83/83 [==============================] - 34s 412ms/step - loss: 0.0726 - accuracy: 0.9917\n",
      "Epoch 32/100\n",
      "83/83 [==============================] - 42s 502ms/step - loss: 0.0553 - accuracy: 0.9940\n",
      "Epoch 33/100\n",
      "83/83 [==============================] - 37s 442ms/step - loss: 0.0508 - accuracy: 0.9947\n",
      "Epoch 34/100\n",
      "83/83 [==============================] - 34s 409ms/step - loss: 0.0506 - accuracy: 0.9936\n",
      "Epoch 35/100\n",
      "83/83 [==============================] - 34s 409ms/step - loss: 0.0465 - accuracy: 0.9924\n",
      "Epoch 36/100\n",
      "83/83 [==============================] - 34s 410ms/step - loss: 0.0421 - accuracy: 0.9932\n",
      "Epoch 37/100\n",
      "83/83 [==============================] - 35s 426ms/step - loss: 0.0393 - accuracy: 0.9940\n",
      "Epoch 38/100\n",
      "83/83 [==============================] - 34s 413ms/step - loss: 0.0367 - accuracy: 0.9943\n",
      "Epoch 39/100\n",
      "83/83 [==============================] - 34s 406ms/step - loss: 0.0348 - accuracy: 0.9955\n",
      "Epoch 40/100\n",
      "83/83 [==============================] - 33s 397ms/step - loss: 0.0300 - accuracy: 0.9955\n",
      "Epoch 41/100\n",
      "83/83 [==============================] - 33s 401ms/step - loss: 0.0305 - accuracy: 0.9947\n",
      "Epoch 42/100\n",
      "83/83 [==============================] - 34s 407ms/step - loss: 0.0258 - accuracy: 0.9947\n",
      "Epoch 43/100\n",
      "83/83 [==============================] - 33s 403ms/step - loss: 0.0285 - accuracy: 0.9951\n",
      "Epoch 44/100\n",
      "83/83 [==============================] - 34s 404ms/step - loss: 0.0234 - accuracy: 0.9962\n",
      "Epoch 45/100\n",
      "83/83 [==============================] - 33s 399ms/step - loss: 0.0233 - accuracy: 0.9958\n",
      "Epoch 46/100\n",
      "83/83 [==============================] - 34s 407ms/step - loss: 0.0226 - accuracy: 0.9974\n",
      "Epoch 47/100\n",
      "83/83 [==============================] - 33s 398ms/step - loss: 0.0221 - accuracy: 0.9962\n",
      "Epoch 48/100\n",
      "83/83 [==============================] - 34s 406ms/step - loss: 0.0198 - accuracy: 0.9966\n",
      "Epoch 49/100\n",
      "83/83 [==============================] - 33s 398ms/step - loss: 0.0206 - accuracy: 0.9962\n",
      "Epoch 50/100\n",
      "83/83 [==============================] - 33s 397ms/step - loss: 0.0199 - accuracy: 0.9955\n",
      "Epoch 51/100\n",
      "83/83 [==============================] - 33s 398ms/step - loss: 0.0181 - accuracy: 0.9966\n",
      "Epoch 52/100\n",
      "83/83 [==============================] - 33s 398ms/step - loss: 0.0159 - accuracy: 0.9970\n",
      "Epoch 53/100\n",
      "83/83 [==============================] - 33s 397ms/step - loss: 0.0146 - accuracy: 0.9970\n",
      "Epoch 54/100\n",
      "83/83 [==============================] - 33s 400ms/step - loss: 0.0139 - accuracy: 0.9970\n",
      "Epoch 55/100\n",
      "83/83 [==============================] - 33s 400ms/step - loss: 0.0151 - accuracy: 0.9966\n",
      "Epoch 56/100\n",
      "83/83 [==============================] - 34s 405ms/step - loss: 0.0166 - accuracy: 0.9962\n",
      "Epoch 57/100\n",
      "83/83 [==============================] - 39s 466ms/step - loss: 0.0159 - accuracy: 0.9966\n",
      "Epoch 58/100\n",
      "83/83 [==============================] - 33s 400ms/step - loss: 0.0170 - accuracy: 0.9958\n",
      "Epoch 59/100\n",
      "83/83 [==============================] - 34s 411ms/step - loss: 0.0115 - accuracy: 0.9977\n",
      "Epoch 60/100\n",
      "83/83 [==============================] - 37s 437ms/step - loss: 0.0140 - accuracy: 0.9974\n",
      "Epoch 61/100\n",
      "83/83 [==============================] - 33s 401ms/step - loss: 0.0129 - accuracy: 0.9970\n",
      "Epoch 62/100\n",
      "83/83 [==============================] - 33s 398ms/step - loss: 0.0141 - accuracy: 0.9966\n",
      "Epoch 63/100\n",
      "83/83 [==============================] - 33s 400ms/step - loss: 0.0144 - accuracy: 0.9966\n",
      "Epoch 64/100\n",
      "83/83 [==============================] - 34s 404ms/step - loss: 0.0128 - accuracy: 0.9966\n",
      "Epoch 65/100\n",
      "83/83 [==============================] - 34s 407ms/step - loss: 0.0127 - accuracy: 0.9966\n",
      "Epoch 66/100\n",
      "83/83 [==============================] - 33s 400ms/step - loss: 0.9741 - accuracy: 0.7789\n",
      "Epoch 67/100\n",
      "83/83 [==============================] - 34s 404ms/step - loss: 3.2042 - accuracy: 0.3401\n",
      "Epoch 68/100\n",
      "83/83 [==============================] - 33s 401ms/step - loss: 1.3248 - accuracy: 0.6500\n",
      "Epoch 69/100\n",
      "83/83 [==============================] - 33s 398ms/step - loss: 0.5924 - accuracy: 0.8390\n",
      "Epoch 70/100\n",
      "83/83 [==============================] - 33s 403ms/step - loss: 0.2266 - accuracy: 0.9361\n",
      "Epoch 71/100\n",
      "83/83 [==============================] - 33s 399ms/step - loss: 0.1140 - accuracy: 0.9739\n",
      "Epoch 72/100\n",
      "83/83 [==============================] - 36s 430ms/step - loss: 0.0593 - accuracy: 0.9875\n",
      "Epoch 73/100\n",
      "83/83 [==============================] - 36s 433ms/step - loss: 0.0325 - accuracy: 0.9943\n",
      "Epoch 74/100\n",
      "83/83 [==============================] - 35s 417ms/step - loss: 0.0238 - accuracy: 0.9947\n",
      "Epoch 75/100\n",
      "83/83 [==============================] - 36s 434ms/step - loss: 0.0206 - accuracy: 0.9962\n",
      "Epoch 76/100\n",
      "83/83 [==============================] - 36s 436ms/step - loss: 0.0171 - accuracy: 0.9958\n",
      "Epoch 77/100\n",
      "83/83 [==============================] - 34s 408ms/step - loss: 0.0164 - accuracy: 0.9955\n",
      "Epoch 78/100\n",
      "83/83 [==============================] - 34s 415ms/step - loss: 0.0150 - accuracy: 0.9958\n",
      "Epoch 79/100\n",
      "83/83 [==============================] - 34s 408ms/step - loss: 0.0143 - accuracy: 0.9962\n",
      "Epoch 80/100\n",
      "83/83 [==============================] - 34s 407ms/step - loss: 0.0130 - accuracy: 0.9962\n",
      "Epoch 81/100\n",
      "83/83 [==============================] - 36s 428ms/step - loss: 0.0126 - accuracy: 0.9958\n",
      "Epoch 82/100\n",
      "83/83 [==============================] - 34s 415ms/step - loss: 0.0131 - accuracy: 0.9966\n",
      "Epoch 83/100\n",
      "83/83 [==============================] - 34s 410ms/step - loss: 0.0121 - accuracy: 0.9966\n",
      "Epoch 84/100\n",
      "83/83 [==============================] - 33s 403ms/step - loss: 0.0147 - accuracy: 0.9958\n",
      "Epoch 85/100\n",
      "83/83 [==============================] - 33s 403ms/step - loss: 0.0122 - accuracy: 0.9962\n",
      "Epoch 86/100\n",
      "83/83 [==============================] - 34s 406ms/step - loss: 0.0104 - accuracy: 0.9962\n",
      "Epoch 87/100\n",
      "83/83 [==============================] - 34s 412ms/step - loss: 0.0098 - accuracy: 0.9970\n",
      "Epoch 88/100\n",
      "83/83 [==============================] - 34s 404ms/step - loss: 0.0090 - accuracy: 0.9970\n",
      "Epoch 89/100\n",
      "83/83 [==============================] - 32s 386ms/step - loss: 0.0085 - accuracy: 0.9974\n",
      "Epoch 90/100\n",
      "83/83 [==============================] - 33s 401ms/step - loss: 0.0090 - accuracy: 0.9981\n",
      "Epoch 91/100\n",
      "83/83 [==============================] - 37s 443ms/step - loss: 0.0089 - accuracy: 0.9970\n",
      "Epoch 92/100\n",
      "83/83 [==============================] - 31s 377ms/step - loss: 0.0080 - accuracy: 0.9977\n",
      "Epoch 93/100\n",
      "83/83 [==============================] - 35s 421ms/step - loss: 0.0086 - accuracy: 0.9966\n",
      "Epoch 94/100\n",
      "83/83 [==============================] - 34s 404ms/step - loss: 0.0077 - accuracy: 0.9974\n",
      "Epoch 95/100\n",
      "83/83 [==============================] - 33s 403ms/step - loss: 0.0081 - accuracy: 0.9977\n",
      "Epoch 96/100\n",
      "83/83 [==============================] - 34s 404ms/step - loss: 0.0084 - accuracy: 0.9974\n",
      "Epoch 97/100\n",
      "83/83 [==============================] - 35s 417ms/step - loss: 0.0072 - accuracy: 0.9977\n",
      "Epoch 98/100\n",
      "83/83 [==============================] - 33s 402ms/step - loss: 0.0075 - accuracy: 0.9970\n",
      "Epoch 99/100\n",
      "83/83 [==============================] - 34s 408ms/step - loss: 0.0073 - accuracy: 0.9966\n",
      "Epoch 100/100\n",
      "83/83 [==============================] - 34s 406ms/step - loss: 0.0071 - accuracy: 0.9970\n",
      "<keras.engine.sequential.Sequential object at 0x7f4640361c40>\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_length-1))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "history = model.fit(xs, ys, epochs=100, verbose=1)\n",
    "#print model.summary()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think it is time to hire you had sex our hr asked a very attractive female candidate during an interview.she applied for an entry level role a few years back. sir i am married with two beautiful daughters. since we migrated here i've been unemployed for three years. i want to work and support my husband who's also out of work. we need financial help. i heard the noise from my office and quickly abandoned a meeting with my partner.we dashed into his office and saw the lady looking so angry and disappointed.i had no idea what happened there so i asked her. she stammered when she tried to speak. it was apparent that she was agitated.we calmed her down. then she opened up your hr is harassing me. he asked too many uncomfortable questions like when was the last time i had sex. i was shocked. i have never witnessed such a thing in my entire life. what has come over you have you forgotten this is an office my partner was mad at him as well.we apologized to the young lady and fired our hr immediately.we rescheduled the interview. she came back for the interview and did well. we gave her the contract.why\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"I think it is time to\"\n",
    "next_words = 200\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list),axis=1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
