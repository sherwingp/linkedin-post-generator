{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read post data into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('influencers_data.csv')\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('...see more', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = \" \".join(filter(lambda x:x[0]!=\"@\", text.split()))\n",
    "    return text\n",
    "\n",
    "corpus = data['content'].apply(lambda x: clean_text(x)).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    filters='\"$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Turn tokenized sentences into training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad Input Sequences\n",
    "max_sequence_length = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre'))\n",
    "\n",
    "# Create predictors and labels\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wingp/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2022-02-28 04:18:33.961984: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 690113880 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "879/879 [==============================] - 247s 277ms/step - loss: 7.1456 - accuracy: 0.0582\n",
      "Epoch 2/100\n",
      "879/879 [==============================] - 232s 264ms/step - loss: 6.2700 - accuracy: 0.0950\n",
      "Epoch 3/100\n",
      "879/879 [==============================] - 230s 262ms/step - loss: 5.7360 - accuracy: 0.1192\n",
      "Epoch 4/100\n",
      "879/879 [==============================] - 243s 276ms/step - loss: 4.6297 - accuracy: 0.1741\n",
      "Epoch 5/100\n",
      "879/879 [==============================] - 238s 271ms/step - loss: 3.7815 - accuracy: 0.2541\n",
      "Epoch 6/100\n",
      "879/879 [==============================] - 249s 283ms/step - loss: 3.2274 - accuracy: 0.3214\n",
      "Epoch 7/100\n",
      "879/879 [==============================] - 241s 275ms/step - loss: 2.8330 - accuracy: 0.3731\n",
      "Epoch 8/100\n",
      "879/879 [==============================] - 238s 271ms/step - loss: 2.5655 - accuracy: 0.4181\n",
      "Epoch 9/100\n",
      "879/879 [==============================] - 262s 298ms/step - loss: 2.3296 - accuracy: 0.4578\n",
      "Epoch 10/100\n",
      "879/879 [==============================] - 269s 306ms/step - loss: 2.1934 - accuracy: 0.4802\n",
      "Epoch 11/100\n",
      "879/879 [==============================] - 272s 310ms/step - loss: 2.0551 - accuracy: 0.5054\n",
      "Epoch 12/100\n",
      "879/879 [==============================] - 287s 327ms/step - loss: 1.9498 - accuracy: 0.5229\n",
      "Epoch 13/100\n",
      "879/879 [==============================] - 254s 289ms/step - loss: 1.8860 - accuracy: 0.5341\n",
      "Epoch 14/100\n",
      "879/879 [==============================] - 253s 288ms/step - loss: 1.8144 - accuracy: 0.5488\n",
      "Epoch 15/100\n",
      "879/879 [==============================] - 262s 298ms/step - loss: 1.7698 - accuracy: 0.5549\n",
      "Epoch 16/100\n",
      "879/879 [==============================] - 271s 308ms/step - loss: 1.7578 - accuracy: 0.5592\n",
      "Epoch 17/100\n",
      "879/879 [==============================] - 265s 301ms/step - loss: 1.6944 - accuracy: 0.5718\n",
      "Epoch 18/100\n",
      "879/879 [==============================] - 276s 314ms/step - loss: 1.6183 - accuracy: 0.5893\n",
      "Epoch 19/100\n",
      "879/879 [==============================] - 277s 315ms/step - loss: 1.5993 - accuracy: 0.5924\n",
      "Epoch 20/100\n",
      "879/879 [==============================] - 227s 258ms/step - loss: 1.5975 - accuracy: 0.5906\n",
      "Epoch 21/100\n",
      "879/879 [==============================] - 218s 247ms/step - loss: 1.5951 - accuracy: 0.5929\n",
      "Epoch 22/100\n",
      "879/879 [==============================] - 217s 247ms/step - loss: 1.5554 - accuracy: 0.5995\n",
      "Epoch 23/100\n",
      "879/879 [==============================] - 215s 245ms/step - loss: 1.5694 - accuracy: 0.5976\n",
      "Epoch 24/100\n",
      "879/879 [==============================] - 215s 244ms/step - loss: 1.5614 - accuracy: 0.5998\n",
      "Epoch 25/100\n",
      "879/879 [==============================] - 215s 244ms/step - loss: 1.5183 - accuracy: 0.6096\n",
      "Epoch 26/100\n",
      "879/879 [==============================] - 214s 244ms/step - loss: 1.4999 - accuracy: 0.6113\n",
      "Epoch 27/100\n",
      "879/879 [==============================] - 215s 244ms/step - loss: 1.5199 - accuracy: 0.6091\n",
      "Epoch 28/100\n",
      "879/879 [==============================] - 34459s 39s/step - loss: 1.4917 - accuracy: 0.6140\n",
      "Epoch 29/100\n",
      "879/879 [==============================] - 262s 298ms/step - loss: 1.4616 - accuracy: 0.6199\n",
      "Epoch 30/100\n",
      "879/879 [==============================] - 254s 289ms/step - loss: 1.4037 - accuracy: 0.6350\n",
      "Epoch 31/100\n",
      "879/879 [==============================] - 250s 285ms/step - loss: 1.4294 - accuracy: 0.6279\n",
      "Epoch 32/100\n",
      "879/879 [==============================] - 273s 311ms/step - loss: 1.4672 - accuracy: 0.6198\n",
      "Epoch 33/100\n",
      "879/879 [==============================] - 285s 324ms/step - loss: 1.4426 - accuracy: 0.6228\n",
      "Epoch 34/100\n",
      "879/879 [==============================] - 286s 325ms/step - loss: 1.4772 - accuracy: 0.6208\n",
      "Epoch 35/100\n",
      "879/879 [==============================] - 298s 339ms/step - loss: 1.4498 - accuracy: 0.6203\n",
      "Epoch 36/100\n",
      "879/879 [==============================] - 364s 414ms/step - loss: 1.4199 - accuracy: 0.6274\n",
      "Epoch 37/100\n",
      "879/879 [==============================] - 357s 406ms/step - loss: 1.4165 - accuracy: 0.6320\n",
      "Epoch 38/100\n",
      "879/879 [==============================] - 337s 383ms/step - loss: 1.3888 - accuracy: 0.6375\n",
      "Epoch 39/100\n",
      "879/879 [==============================] - 345s 393ms/step - loss: 1.3959 - accuracy: 0.6373\n",
      "Epoch 40/100\n",
      "879/879 [==============================] - 335s 381ms/step - loss: 1.3762 - accuracy: 0.6366\n",
      "Epoch 41/100\n",
      "879/879 [==============================] - 361s 411ms/step - loss: 1.4014 - accuracy: 0.6322\n",
      "Epoch 42/100\n",
      "879/879 [==============================] - 306s 348ms/step - loss: 1.4339 - accuracy: 0.6287\n",
      "Epoch 43/100\n",
      "879/879 [==============================] - 349s 397ms/step - loss: 1.4592 - accuracy: 0.6221\n",
      "Epoch 44/100\n",
      "879/879 [==============================] - 342s 390ms/step - loss: 1.3249 - accuracy: 0.6521\n",
      "Epoch 45/100\n",
      "879/879 [==============================] - 313s 356ms/step - loss: 1.2876 - accuracy: 0.6578\n",
      "Epoch 46/100\n",
      "879/879 [==============================] - 341s 387ms/step - loss: 1.3188 - accuracy: 0.6543\n",
      "Epoch 47/100\n",
      "879/879 [==============================] - 274s 311ms/step - loss: 1.3292 - accuracy: 0.6493\n",
      "Epoch 48/100\n",
      "879/879 [==============================] - 252s 287ms/step - loss: 1.3453 - accuracy: 0.6448\n",
      "Epoch 49/100\n",
      "879/879 [==============================] - 254s 289ms/step - loss: 1.3080 - accuracy: 0.6555\n",
      "Epoch 50/100\n",
      "879/879 [==============================] - 250s 285ms/step - loss: 1.3245 - accuracy: 0.6520\n",
      "Epoch 51/100\n",
      "879/879 [==============================] - 252s 287ms/step - loss: 1.3525 - accuracy: 0.6448\n",
      "Epoch 52/100\n",
      "879/879 [==============================] - 253s 288ms/step - loss: 1.3382 - accuracy: 0.6480\n",
      "Epoch 53/100\n",
      "879/879 [==============================] - 251s 286ms/step - loss: 1.3128 - accuracy: 0.6544\n",
      "Epoch 54/100\n",
      "879/879 [==============================] - 353s 402ms/step - loss: 1.2860 - accuracy: 0.6603\n",
      "Epoch 55/100\n",
      "879/879 [==============================] - 303s 345ms/step - loss: 1.2781 - accuracy: 0.6615\n",
      "Epoch 56/100\n",
      "879/879 [==============================] - 254s 288ms/step - loss: 1.3117 - accuracy: 0.6538\n",
      "Epoch 57/100\n",
      "879/879 [==============================] - 339s 385ms/step - loss: 1.3184 - accuracy: 0.6513\n",
      "Epoch 58/100\n",
      "879/879 [==============================] - 253s 288ms/step - loss: 1.3189 - accuracy: 0.6529\n",
      "Epoch 59/100\n",
      "879/879 [==============================] - 303s 344ms/step - loss: 1.2853 - accuracy: 0.6601\n",
      "Epoch 60/100\n",
      "879/879 [==============================] - 257s 292ms/step - loss: 1.2670 - accuracy: 0.6619\n",
      "Epoch 61/100\n",
      "879/879 [==============================] - 349s 397ms/step - loss: 1.2585 - accuracy: 0.6671\n",
      "Epoch 62/100\n",
      "879/879 [==============================] - 293s 334ms/step - loss: 1.2792 - accuracy: 0.6633\n",
      "Epoch 63/100\n",
      "879/879 [==============================] - 270s 307ms/step - loss: 1.2840 - accuracy: 0.6652\n",
      "Epoch 64/100\n",
      "879/879 [==============================] - 300s 341ms/step - loss: 1.3226 - accuracy: 0.6544\n",
      "Epoch 65/100\n",
      "879/879 [==============================] - 252s 287ms/step - loss: 1.3227 - accuracy: 0.6530\n",
      "Epoch 66/100\n",
      "879/879 [==============================] - 305s 347ms/step - loss: 1.3062 - accuracy: 0.6578\n",
      "Epoch 67/100\n",
      "879/879 [==============================] - 274s 311ms/step - loss: 1.2887 - accuracy: 0.6616\n",
      "Epoch 68/100\n",
      "879/879 [==============================] - 306s 348ms/step - loss: 1.2913 - accuracy: 0.6605\n",
      "Epoch 69/100\n",
      "879/879 [==============================] - 347s 395ms/step - loss: 1.2593 - accuracy: 0.6661\n",
      "Epoch 70/100\n",
      "879/879 [==============================] - 350s 399ms/step - loss: 1.2375 - accuracy: 0.6758\n",
      "Epoch 71/100\n",
      "879/879 [==============================] - 257s 292ms/step - loss: 1.2561 - accuracy: 0.6731\n",
      "Epoch 72/100\n",
      "879/879 [==============================] - 302s 344ms/step - loss: 1.2510 - accuracy: 0.6690\n",
      "Epoch 73/100\n",
      "879/879 [==============================] - 285s 324ms/step - loss: 1.2617 - accuracy: 0.6678\n",
      "Epoch 74/100\n",
      "879/879 [==============================] - 331s 377ms/step - loss: 1.2693 - accuracy: 0.6670\n",
      "Epoch 75/100\n",
      "879/879 [==============================] - 327s 372ms/step - loss: 1.2111 - accuracy: 0.6774\n",
      "Epoch 76/100\n",
      "879/879 [==============================] - 370s 421ms/step - loss: 1.2014 - accuracy: 0.6821\n",
      "Epoch 77/100\n",
      "879/879 [==============================] - 308s 350ms/step - loss: 1.1807 - accuracy: 0.6869\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879/879 [==============================] - 235s 268ms/step - loss: 1.2412 - accuracy: 0.6755\n",
      "Epoch 79/100\n",
      "879/879 [==============================] - 277s 316ms/step - loss: 1.2781 - accuracy: 0.6653\n",
      "Epoch 80/100\n",
      "879/879 [==============================] - 257s 292ms/step - loss: 1.2473 - accuracy: 0.6743\n",
      "Epoch 81/100\n",
      "879/879 [==============================] - 295s 336ms/step - loss: 1.2143 - accuracy: 0.6787\n",
      "Epoch 82/100\n",
      "879/879 [==============================] - 304s 345ms/step - loss: 1.1725 - accuracy: 0.6862\n",
      "Epoch 83/100\n",
      "879/879 [==============================] - 338s 384ms/step - loss: 1.1786 - accuracy: 0.6879\n",
      "Epoch 84/100\n",
      "879/879 [==============================] - 276s 314ms/step - loss: 1.2106 - accuracy: 0.6815\n",
      "Epoch 85/100\n",
      "879/879 [==============================] - 252s 286ms/step - loss: 1.2461 - accuracy: 0.6744\n",
      "Epoch 86/100\n",
      "879/879 [==============================] - 298s 340ms/step - loss: 1.1894 - accuracy: 0.6861\n",
      "Epoch 87/100\n",
      "879/879 [==============================] - 278s 316ms/step - loss: 1.1564 - accuracy: 0.6975\n",
      "Epoch 88/100\n",
      "879/879 [==============================] - 263s 300ms/step - loss: 1.1506 - accuracy: 0.6976\n",
      "Epoch 89/100\n",
      "879/879 [==============================] - 268s 304ms/step - loss: 1.1528 - accuracy: 0.6915\n",
      "Epoch 90/100\n",
      "879/879 [==============================] - 262s 298ms/step - loss: 1.1882 - accuracy: 0.6877\n",
      "Epoch 91/100\n",
      "879/879 [==============================] - 273s 311ms/step - loss: 1.1463 - accuracy: 0.6945\n",
      "Epoch 92/100\n",
      "879/879 [==============================] - 314s 357ms/step - loss: 1.1251 - accuracy: 0.6997\n",
      "Epoch 93/100\n",
      "879/879 [==============================] - 272s 309ms/step - loss: 1.1286 - accuracy: 0.6984\n",
      "Epoch 94/100\n",
      "879/879 [==============================] - 284s 323ms/step - loss: 1.1547 - accuracy: 0.6943\n",
      "Epoch 95/100\n",
      "879/879 [==============================] - 274s 311ms/step - loss: 1.1436 - accuracy: 0.6957\n",
      "Epoch 96/100\n",
      "879/879 [==============================] - 257s 292ms/step - loss: 1.1425 - accuracy: 0.6967\n",
      "Epoch 97/100\n",
      "879/879 [==============================] - 286s 325ms/step - loss: 1.2307 - accuracy: 0.6822\n",
      "Epoch 98/100\n",
      "879/879 [==============================] - 356s 406ms/step - loss: 1.1959 - accuracy: 0.6861\n",
      "Epoch 99/100\n",
      "879/879 [==============================] - 288s 328ms/step - loss: 1.1137 - accuracy: 0.7031\n",
      "Epoch 100/100\n",
      "879/879 [==============================] - 283s 322ms/step - loss: 1.0759 - accuracy: 0.7091\n",
      "<keras.engine.sequential.Sequential object at 0x7f0791917550>\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_length-1))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "history = model.fit(xs, ys, epochs=100, verbose=1)\n",
    "#print model.summary()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-28 21:40:31.711548: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f0791358b80> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f079135ffa0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "!mkdir -p saved_model\n",
    "model.save('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We need to have ownership of the life i have been focusing on the process of my life with some of the coolest of problems. but do you prioritize #founders #scale #advantage both and agreed dates for some people or passionately want he who are late in the future â€˜re shoot hundreds of\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"We need to\"\n",
    "next_words = 50\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list),axis=1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "    \n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
